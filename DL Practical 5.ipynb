{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662d5ff1",
   "metadata": {},
   "source": [
    "# Implement the Continuous Bag of Words (CBOW) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2217de6",
   "metadata": {},
   "source": [
    "a) Data Preparation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de790329",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"I like to learn deep learning\",\n",
    "    \"Deep learning is interesting\",\n",
    "    \"I enjoy studying deep learning\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec79a012",
   "metadata": {},
   "source": [
    "b) Generate Training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae680d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer is to split paragraph into small unit\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# skipgram is to skipped token\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "# Generate training data\n",
    "vocab_size = len(word2idx) + 1\n",
    "target_words, context_words = [], []\n",
    "# for control flow statement\n",
    "for sentence in corpus:\n",
    "    tokenized = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    pairs, _ = skipgrams(tokenized, vocabulary_size=vocab_size, window_size=1, negative_samples=0)\n",
    "    target, context = zip(*pairs)\n",
    "    target_words.extend(target)\n",
    "    context_words.extend(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a4e12",
   "metadata": {},
   "source": [
    "c) Train Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89f7be1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1, 100)            1100      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 11)                1111      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2211 (8.64 KB)\n",
      "Trainable params: 2211 (8.64 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 2.4044\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3951\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3858\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3766\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3673\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3581\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3488\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3396\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3303\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3210\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3118\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3025\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2931\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2838\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2744\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2649\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2554\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2459\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2363\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2266\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2169\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2071\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.1973\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1873\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1773\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1672\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1570\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.1467\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1363\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1258\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.1151\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.1044\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.0936\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0827\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0716\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0604\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0491\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0377\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0262\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0146\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0028\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9909\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.9789\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9668\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9545\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9422\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9297\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9171\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9044\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23bb986a590>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensorflow is library for ML for training deep NN\n",
    "import tensorflow as tf\n",
    "# Sequentials is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "from tensorflow.keras.models import Sequential\n",
    "# reshape give shape dense feed output from previous layer embedding is implement functionality\n",
    "from tensorflow.keras.layers import Embedding, Dense, Reshape\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1))\n",
    "model.add(Reshape((embedding_dim,)))\n",
    "model.add(Dense(units=vocab_size, activation='softmax'))\n",
    "\n",
    "#adam is optimization of stochastic gradient descent\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "# epochs is nn that train data for one cycle\n",
    "model.fit(x=target_words, y=context_words, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af00f55",
   "metadata": {},
   "source": [
    "d) Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6dbed69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'deep': [ 8.41328874e-02  5.61344139e-02 -5.13058566e-02 -7.88685083e-02\n",
      " -9.91607383e-02  2.61893086e-02  2.06635892e-02 -6.09128252e-02\n",
      " -3.73820066e-02  5.98195046e-02 -5.57249784e-03  8.60996023e-02\n",
      "  2.02384237e-02 -9.22028869e-02  6.75316453e-02  9.99911204e-02\n",
      "  2.28100065e-02 -4.97749075e-02 -6.98095113e-02  7.34467953e-02\n",
      " -5.73070869e-02 -2.31165849e-02 -4.10012715e-03 -4.91816700e-02\n",
      "  6.94095641e-02  2.69928086e-03  4.69533214e-03  9.61602200e-03\n",
      " -3.08772596e-03 -5.56841753e-02  3.45945656e-02 -7.15998039e-02\n",
      "  9.93780419e-02  8.83758292e-02 -5.64689375e-02  7.85832927e-02\n",
      "  1.78814139e-02  6.97733238e-02 -7.09630502e-03 -8.90499502e-02\n",
      "  9.29317400e-02  3.82730849e-02 -4.85376781e-03  1.84024461e-02\n",
      "  2.18894016e-02  2.34995678e-05  8.21887329e-02  1.04050953e-02\n",
      " -7.20224436e-03  1.20419599e-02  1.87101327e-02  6.81096166e-02\n",
      " -7.08519947e-03 -8.06856677e-02  1.00942455e-01  9.44088697e-02\n",
      "  4.73482609e-02 -3.77868675e-02  5.39150536e-02  4.03142301e-03\n",
      "  8.97629594e-04 -8.72991011e-02 -7.19413161e-02 -5.77466339e-02\n",
      "  1.52609758e-02  2.79204212e-02  3.31328362e-02 -5.08924201e-02\n",
      "  3.50671122e-03  4.25736494e-02  6.14874102e-02 -2.13844329e-02\n",
      " -6.88826367e-02  8.68476555e-02  3.72614786e-02  6.05603792e-02\n",
      "  9.32395384e-02  9.43349153e-02 -1.09016284e-01  2.64159907e-02\n",
      "  6.65666685e-02 -2.22381335e-02  1.68145653e-02 -1.11089442e-02\n",
      "  1.34845581e-02  8.56811330e-02 -3.34836319e-02  1.03432477e-01\n",
      " -1.41639984e-03  8.83791223e-02  1.84661173e-03  9.78341103e-02\n",
      " -4.99576293e-02 -1.09633189e-02 -7.32791647e-02  6.86876327e-02\n",
      "  6.60399348e-02  1.29931075e-02 -1.66293327e-02  7.89908692e-02]\n"
     ]
    }
   ],
   "source": [
    "word_to_lookup = \"deep\"\n",
    "word_idx = word2idx[word_to_lookup]\n",
    "word_embedding = model.layers[0].get_weights()[0][word_idx]\n",
    "\n",
    "print(f\"Embedding for '{word_to_lookup}': {word_embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73c43bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
